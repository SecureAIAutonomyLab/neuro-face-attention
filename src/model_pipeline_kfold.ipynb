{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
    "warning_status = \"ignore\" #@param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(warning_status)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
    "    warnings.filterwarnings(warning_status, category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input, Reshape, Permute\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# to load individual subjects\n",
    "subjects = ['942', '1131', '1196', '1214', '970', '971', '982']\n",
    "\n",
    "X_array = []\n",
    "Y_array = []\n",
    "\n",
    "if len(subjects) > 1:\n",
    "    for subject in subjects:\n",
    "        with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: X_array.append(pickle.load(f))\n",
    "        with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: Y_array.append(pickle.load(f))\n",
    "    X_array = np.vstack(X_array)\n",
    "    Y_array = np.hstack(Y_array)\n",
    "else:\n",
    "    with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "        \n",
    "print(X_array.shape, Y_array.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced\n",
    "paradigm = 'all'\n",
    "\n",
    "if paradigm=='all':\n",
    "    # All Paradigms\n",
    "    print(\"All Paradigms\")\n",
    "    with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "elif paradigm=='cw':\n",
    "    # CueWord\n",
    "    print(\"Cue-Word Paradigm\")\n",
    "    with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "elif paradigm=='wg':\n",
    "    # WordGo\n",
    "    print(\"Word-Go Paradigm\")\n",
    "    with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_WordGo_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_WordGo_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "print(\"X_array.shape: \", X_array.shape)\n",
    "Y_hist = np.histogram(Y_array)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# to load unbalanced dataset\n",
    "# Unbalanced\n",
    "paradigm = 'all'\n",
    "\n",
    "if paradigm=='all':\n",
    "    # All Paradigms\n",
    "    print(\"All Paradigms\")\n",
    "    with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "elif paradigm=='cw':\n",
    "    # CueWord\n",
    "    print(\"Cue-Word Paradigm\")\n",
    "    with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "elif paradigm=='wg':\n",
    "    # WordGo\n",
    "    print(\"Word-Go Paradigm\")\n",
    "    with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_CueWord_all_subjects.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "\n",
    "print(\"X_array.shape: \", X_array.shape)\n",
    "Y_hist = np.histogram(Y_array)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _StutterNet_C_S1S2. Functional API model.\n",
    "def make_balanced(X,Y):\n",
    "    \"Binary label balancer function\"\n",
    "    try:\n",
    "        # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "        # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "        # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "        # later we concatenate both data points to create a balanced dataset.\n",
    "        random_data_points = np.random.choice(np.where(Y==0)[0], size=X[Y==1].shape[0], replace=False)\n",
    "        assert random_data_points.shape[0] == X[Y==1].shape[0]\n",
    "        random_data_points = np.concatenate((random_data_points, np.where(Y==1)[0]))\n",
    "        random.shuffle(random_data_points)\n",
    "\n",
    "    except ValueError:\n",
    "        # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "        random_data_points = np.random.choice(np.where(Y==1)[0], size=X[Y==0].shape[0], replace=False)\n",
    "        assert random_data_points.shape[0] == X[Y==0].shape[0]\n",
    "        random_data_points = np.concatenate((random_data_points, np.where(Y==0)[0]))\n",
    "        random.shuffle(random_data_points)\n",
    "    return(X[random_data_points], Y[random_data_points])\n",
    "\n",
    "def define_model(dropout=0.25):\n",
    "    K.set_image_data_format('channels_first')\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    channels = 17\n",
    "    timesteps = 87 # upsampled 58 fps\n",
    "\n",
    "    inputs = Input(shape=(channels, timesteps))\n",
    "\n",
    "    input_permute = Permute((1, 2), input_shape=(channels, timesteps))(inputs)\n",
    "    input_reshape = Reshape((1, channels, timesteps))(input_permute)\n",
    "\n",
    "    conv2d_1 = Conv2D(32, (1,channels), activation='linear', input_shape=(channels, timesteps), padding='same')(input_reshape)\n",
    "    conv2d_1_bn = BatchNormalization()(conv2d_1)\n",
    "\n",
    "    conv2d_2DW = DepthwiseConv2D((channels,1), use_bias=False, activation='linear', depth_multiplier=2, padding='valid', kernel_constraint=max_norm(1.))(conv2d_1_bn)\n",
    "    conv2d_2DW_bn = BatchNormalization()(conv2d_2DW)\n",
    "    conv2d_2DW_bn_act = Activation('elu')(conv2d_2DW_bn)\n",
    "\n",
    "    conv2d_2DW_bn_act_avpool = AveragePooling2D((1,4))(conv2d_2DW_bn_act)\n",
    "    conv2d_2DW_bn_act_avpool_dp = Dropout(rate=dropout)(conv2d_2DW_bn_act_avpool)\n",
    "\n",
    "    conv2d_3Sep = SeparableConv2D(32, (1, channels-1), activation='linear', padding='same')(conv2d_2DW_bn_act_avpool_dp)\n",
    "    conv2d_3Sep_bn = BatchNormalization()(conv2d_3Sep)\n",
    "    conv2d_3Sep_bn_act = Activation('elu')(conv2d_3Sep_bn)\n",
    "\n",
    "    conv2d_3Sep_bn_act_avgpool = AveragePooling2D((1,8))(conv2d_3Sep_bn_act)\n",
    "    conv2d_3Sep_bn_act_avgpool_dp = Dropout(rate=dropout)(conv2d_3Sep_bn_act_avgpool)\n",
    "\n",
    "    flatten_1 = Flatten()(conv2d_3Sep_bn_act_avgpool_dp)\n",
    "    dense_1 = Dense(64, activation='elu', \n",
    "                    kernel_constraint=max_norm(0.25),\n",
    "                    name='embedding')(flatten_1)\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid', \n",
    "                        kernel_constraint=max_norm(0.25), \n",
    "                        name='predictions')(dense_1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_auc(fpr, tpr, auc_value):\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='AUC-ROC (area = {:.3f})'.format(auc_value))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "model = define_model()\n",
    "model.summary()\n",
    "# plot_model(model, to_file='StutterNet_C.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_array_, X_test, Y_array_, y_test = train_test_split(\n",
    "    X_array, Y_array, test_size=0.2, random_state=seed)\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "aucs = []\n",
    "count = 1\n",
    "\n",
    "\n",
    "for train, test in kfold.split(X_array_, Y_array_):\n",
    "    print(\"Processing Fold \", count)\n",
    "    \n",
    "    print(\"Train Data Statistics\")\n",
    "    print(\"X_array.shape: \", X_array_[train].shape)\n",
    "    Y_hist = np.histogram(Y_array_[train])\n",
    "    Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "    print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "        \n",
    "    print(\"Validation Data Statistics\")\n",
    "    print(\"X_array.shape: \", X_array_[test].shape)\n",
    "    Y_hist = np.histogram(Y_array_[test])\n",
    "    Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "    print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "    \n",
    "    print(\"Test Data Statistics\")\n",
    "    print(\"X_array.shape: \", X_test.shape)\n",
    "    Y_hist = np.histogram(y_test)\n",
    "    Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "    print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "    count += 1\n",
    "    try:\n",
    "        del(model)\n",
    "    except NameError:\n",
    "        pass\n",
    "      \n",
    "    hyper_parameters = {}\n",
    "    hyper_parameters['lr']=0.01\n",
    "    hyper_parameters['momentum']=0.9\n",
    "    hyper_parameters['lr_factor'] = 0.5\n",
    "    hyper_parameters['lr_patience'] = 150\n",
    "    hyper_parameters['lr_cooldown'] = 150\n",
    "    hyper_parameters['min_lr'] = 0.001\n",
    "    hyper_parameters['early_min_delta'] = 0.0001\n",
    "    hyper_parameters['early_patience'] = 150\n",
    "    hyper_parameters['batch_size'] = 256\n",
    "    hyper_parameters['epochs'] = 1000\n",
    "    hyper_parameters['dropout']=0.5\n",
    "    hyper_parameters['optimizer'] = 'sgd'\n",
    "    hyper_parameters['loss'] = 'binary_crossentropy'\n",
    "    \n",
    "    model = define_model(dropout=hyper_parameters['dropout'])\n",
    "    sgd = optimizers.SGD(lr=hyper_parameters['lr'], momentum=hyper_parameters['momentum'], nesterov=False)\n",
    "    adam = optimizers.Adam(learning_rate=hyper_parameters['lr'], amsgrad=False)\n",
    "    hyper_parameters['optimizer'] = sgd\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
    "                                  factor=hyper_parameters['lr_factor'], \n",
    "                                  patience=hyper_parameters['lr_patience'], \n",
    "                                  cooldown=hyper_parameters['lr_cooldown'], \n",
    "                                  min_lr=hyper_parameters['min_lr'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=hyper_parameters['early_min_delta'], \n",
    "                               patience=hyper_parameters['early_patience'])\n",
    "    \n",
    "    model.compile(loss=hyper_parameters['loss'], \n",
    "                  optimizer=hyper_parameters['optimizer'], \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_array_[train], Y_array_[train], \n",
    "                        epochs=hyper_parameters['epochs'], \n",
    "                        batch_size=hyper_parameters['batch_size'], \n",
    "                        verbose=0,\n",
    "                        validation_data=(X_array_[test], Y_array_[test]),\n",
    "                        shuffle=True,\n",
    "                        callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "    test_output = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = np.round(np.clip(y_pred, 0, 1)).flatten()\n",
    "    precision = precision_score(y_test, y_pred_binary)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    print(\"Accuracy: {:.2f}%, Recall: {:.3f}, Precision: {:.3f}, F1: {:.3f}\".format(test_output[1]*100, recall, precision, f1))\n",
    "    \n",
    "    # AUC ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    auc_dict={}\n",
    "    auc_dict['fpr'] = fpr\n",
    "    auc_dict['tpr'] = tpr\n",
    "    auc_dict['thresholds'] = thresholds\n",
    "    auc_dict['auc'] = auc_value\n",
    "    \n",
    "    plot_auc(fpr, tpr, auc_value)\n",
    "    \n",
    "    accuracies.append(test_output[1]*100)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    aucs.append(auc_value)\n",
    "    \n",
    "    if test_output[1] > 0.77 and recall > 0.62:\n",
    "        model_name = '_StutterNet_C_upscaled_S1S2_kfold_'+ paradigm +'_'\n",
    "        model_path = '../trained_models/' + str(datetime.date.today()) + model_name + '{:.3f}'.format(test_output[1])[-3:]\n",
    "        print(\"Saving to: \", model_path + '.h5')\n",
    "        model.save(model_path + '.h5')\n",
    "        with open(model_path + '_auc_details.pkl','wb') as f: pickle.dump(auc_dict, f)\n",
    "        with open(model_path + '_history.pkl','wb') as f: pickle.dump(model.history.history, f)\n",
    "        with open(model_path + '_params.pkl','wb') as f: pickle.dump(model.history.params, f)\n",
    "        with open(model_path + '_hyperparameters.pkl','wb') as f: pickle.dump(hyper_parameters, f)\n",
    "            \n",
    "    ##     with open(model_path + '_history.pkl','rb') as f: history2 = pickle.load(f)\n",
    "    ##     with open(model_path + '_params.pkl','rb') as f: params2 = pickle.load(f)\n",
    "\n",
    "print(\"Training Statistics\")\n",
    "metrics_df = pd.DataFrame.from_dict({'Accuracy': accuracies, 'F1': f1s, 'Recall': recalls, 'Precision': precisions, 'AUC': aucs})\n",
    "print(metrics_df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
