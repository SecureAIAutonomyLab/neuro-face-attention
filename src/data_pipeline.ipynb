{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape, Permute\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import signal\n",
    "\n",
    "# AU definitions\n",
    "au_columns_name = {}\n",
    "au_columns_name['AU01_r'] = 'Inner brow raiser, upper'\n",
    "au_columns_name['AU02_r'] = 'Outer brow raiser, upper'\n",
    "au_columns_name['AU04_r'] = 'Brow lowerer, upper'\n",
    "au_columns_name['AU05_r'] = 'Upper lid raiser, upper'\n",
    "au_columns_name['AU06_r'] = 'Cheekraiser, upper'\n",
    "au_columns_name['AU07_r'] = 'Lid tightener, upper'\n",
    "au_columns_name['AU09_r'] = 'Nose wrinkler, lower'\n",
    "au_columns_name['AU10_r'] = 'Upper lip raiser, lower'\n",
    "au_columns_name['AU12_r'] = 'Lip corner puller, lower'\n",
    "au_columns_name['AU14_r'] = 'Dimpler, lower'\n",
    "au_columns_name['AU15_r'] = 'Lip corner depressor, lower'\n",
    "au_columns_name['AU17_r'] = 'Chin raiser, lower'\n",
    "au_columns_name['AU20_r'] = 'Lipstretcher, lower'\n",
    "au_columns_name['AU23_r'] = 'Lip tightener, lower'\n",
    "au_columns_name['AU25_r'] = 'Lips part, lower'\n",
    "au_columns_name['AU26_r'] = 'Jaw drop, lower'\n",
    "au_columns_name['AU45_r'] = 'Blink, upper'\n",
    "\n",
    "def generate_dataset(subjects, studies, paradigms, max_sequences=58, stack=False):\n",
    "    \"\"\"Generate Dataset based on resampling method for creating a balanced dataset with the action unit information.\n",
    "    \"\"\"\n",
    "    fps_dataset = pd.read_csv('../dataset/csv_labels/FPS_of_stutter_dataset.csv', index_col=0)\n",
    "    \n",
    "    for subject in subjects:\n",
    "        for study in studies:\n",
    "            for paradigm in paradigms:\n",
    "                global X\n",
    "                global Y\n",
    "                \n",
    "                print(\"{}/{}/{}\".format(subject, study, paradigm))\n",
    "                fps = int(fps_dataset[np.logical_and(np.logical_and(fps_dataset['Paradigm']=='CAW1', fps_dataset['Study']=='S1'), fps_dataset['Subject']==982)][\"FPS\"])\n",
    "                max_fps = int(max(fps_dataset['FPS']))\n",
    "                \n",
    "                try:\n",
    "                    df_with_trials = pd.read_csv(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.lower(), \n",
    "                                                             ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv', sep=',')\n",
    "                except:\n",
    "                    df_with_trials = pd.read_csv(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.upper(), \n",
    "                                                             ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv', sep=',')\n",
    "\n",
    "                try:\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.lower(), ('_').join([subject, paradigm.lower()])]) + '.csv', sep=', ')\n",
    "                except:\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.upper(), ('_').join([subject, paradigm.upper()])]) + '.csv', sep=', ')\n",
    "            \n",
    "                #print(full_au_dataset.head())\n",
    "                just_aus = full_au_dataset[sorted(list(au_columns_name.keys()))]\n",
    "                # print(just_aus.head())\n",
    "                just_aus['sequence'] = just_aus.apply(lambda x: list(x), axis=1)\n",
    "                # just_aus.head()\n",
    "\n",
    "                # minimum number of sequences found from the dataset csv.\n",
    "                min_length = min(df_with_trials['trial_end'] - df_with_trials['trial_start'])\n",
    "                # for all 50 trials per subject\n",
    "                for i in range(50):\n",
    "                    S1 = df_with_trials['trial_start'][i]\n",
    "                    S2 = int(S1 + 1.5*fps)\n",
    "                    signal_df = pd.DataFrame(list(just_aus['sequence'].iloc[S1:S2]))\n",
    "                    signal_df_resampled = signal_df.apply(lambda x: signal.resample(x, int(1.5*max_fps)), axis=0)\n",
    "\n",
    "                    # Round the values to 3 decimals (ablation study for preprocessing maybe?)\n",
    "                    signal_df_resampled = np.round(signal_df_resampled, 3)\n",
    "                    # Convert to numpy array\n",
    "                    signal_array = np.asarray(np.round(signal_df_resampled, 3), np.float32)\n",
    "\n",
    "                    X.append(signal_array)\n",
    "                    Y.append(df_with_trials['code'].iloc[i])\n",
    "                    \n",
    "    if stack:\n",
    "        # Moving axes to DL format NHWC. However, here, we don't consider 'c (channels)'\n",
    "        # because we do the permute and reshape operations during training in Keras.\n",
    "        # Not the best way to do it, but we can always add code to create the dataset properly\n",
    "        # to work with any DL platform. As such the only change needed to this X array\n",
    "        # is to add a new axis for the channels and make it 4D NHWC or NCHW.\n",
    "        X = np.dstack(X)\n",
    "        X = np.moveaxis(X, [0,1], [2,1])\n",
    "\n",
    "def stack_dataset(X):\n",
    "    # Moving axes to DL format NHWC. However, here, we don't consider 'c (channels)'\n",
    "    # because we do the permute and reshape operations during training in Keras.\n",
    "    # Not the best way to do it, but we can always add code to create the dataset properly\n",
    "    # to work with any DL platform. As such the only change needed to this X array\n",
    "    # is to add a new axis for the channels and make it 4D NHWC or NCHW.\n",
    "    X = np.dstack(X)\n",
    "    X = np.moveaxis(X, [0,1], [2,1])\n",
    "    return (X)\n",
    "\n",
    "def correct_dataset_labels(_X, _Y):\n",
    "    list_of_unwanted_indexes = []\n",
    "    for i, y in enumerate(_Y):\n",
    "        # print (i,y)\n",
    "        if y==0:\n",
    "            _Y[i] = 0\n",
    "        if 0 < y < 4:\n",
    "            # print(y, \"Y[i] between 0 and 4. Y[i] = 1\")\n",
    "            _Y[i] = 1\n",
    "        elif y>3:\n",
    "            list_of_unwanted_indexes.append(i)    \n",
    "\n",
    "    _X_ = [v for i,v in enumerate(_X) if i not in list_of_unwanted_indexes]\n",
    "    _Y_ = [v for i,v in enumerate(_Y) if i not in list_of_unwanted_indexes]\n",
    "    return(np.asarray(_X_), np.asarray(_Y_))\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "#Upsampled dataset for 58 fps\n",
    "max_sequences = 442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['971']\n",
    "studies = ['S3', 'S5']\n",
    "paradigms = ['caw1', 'caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms, max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['982']\n",
    "studies = ['S1']\n",
    "paradigms = ['caw1', 'caw2', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms, max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['982']\n",
    "studies = ['S2']\n",
    "paradigms = ['caw2', 'wag1', 'wag2', 'cw2', 'wg1']\n",
    "generate_dataset(subjects, studies, paradigms, max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['982']\n",
    "studies = ['S3']\n",
    "paradigms = ['caw1', 'caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms, max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['1214']\n",
    "studies = ['S1']\n",
    "paradigms = ['caw1', 'caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms, max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "\n",
    "print(X_array_corrected.shape)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "with open('../dataset/pickled_datasets/X_array_raw_upsampled_S1S2.pkl','wb') as f: pickle.dump(np.asarray(X_array, np.float32), f)\n",
    "with open('../dataset/pickled_datasets/Y_array_raw_upsampled_S1S2.pkl','wb') as f: pickle.dump(np.asarray(Y_array, np.float32), f)\n",
    "# label corrected data\n",
    "with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2.pkl','wb') as f: pickle.dump(np.asarray(X_array_corrected, np.float32), f)\n",
    "with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2.pkl','wb') as f: pickle.dump(np.asarray(Y_array_corrected, np.float32), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
