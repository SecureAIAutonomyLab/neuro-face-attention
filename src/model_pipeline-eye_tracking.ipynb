{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input, Reshape, Permute\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AU definitions\n",
    "au_columns_name = {}\n",
    "au_columns_name['AU01_r'] = 'Inner brow raiser, upper'\n",
    "au_columns_name['AU02_r'] = 'Outer brow raiser, upper'\n",
    "au_columns_name['AU04_r'] = 'Brow lowerer, upper'\n",
    "au_columns_name['AU05_r'] = 'Upper lid raiser, upper'\n",
    "au_columns_name['AU06_r'] = 'Cheekraiser, upper'\n",
    "au_columns_name['AU07_r'] = 'Lid tightener, upper'\n",
    "au_columns_name['AU09_r'] = 'Nose wrinkler, lower'\n",
    "au_columns_name['AU10_r'] = 'Upper lip raiser, lower'\n",
    "au_columns_name['AU12_r'] = 'Lip corner puller, lower'\n",
    "au_columns_name['AU14_r'] = 'Dimpler, lower'\n",
    "au_columns_name['AU15_r'] = 'Lip corner depressor, lower'\n",
    "au_columns_name['AU17_r'] = 'Chin raiser, lower'\n",
    "au_columns_name['AU20_r'] = 'Lipstretcher, lower'\n",
    "au_columns_name['AU23_r'] = 'Lip tightener, lower'\n",
    "au_columns_name['AU25_r'] = 'Lips part, lower'\n",
    "au_columns_name['AU26_r'] = 'Jaw drop, lower'\n",
    "au_columns_name['AU45_r'] = 'Blink, upper'\n",
    "\n",
    "fps_df = pd.read_csv('../dataset/csv_labels/FPS_of_stutter_dataset.csv', index_col=0)\n",
    "max_fps = max(fps_df[\"FPS\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../dataset/pickled_datasets/X_array_corrected_downsampled.pkl','rb') as f: X_array = pickle.load(f)\n",
    "with open('../dataset/pickled_datasets/Y_array_corrected_downsampled.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../dataset/pickled_datasets/X_array_corrected_upsampled.pkl','rb') as f: X_array = pickle.load(f)\n",
    "with open('../dataset/pickled_datasets/Y_array_corrected_upsampled.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2.pkl','rb') as f: X_array = pickle.load(f)\n",
    "# with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(matrix_mit):\n",
    "    mit_shape = matrix_mit.shape\n",
    "    data = np.empty((mit_shape[2], mit_shape[1]-2, mit_shape[0]))\n",
    "    for i in range(mit_shape[2]):\n",
    "        dat_t = np.squeeze(matrix_mit[:, 1:6, i]).transpose()\n",
    "        data[i, :, :] = (dat_t - dat_t.min())/(dat_t.max()-dat_t.min())\n",
    "    return data\n",
    "\n",
    "mat_all_c_stutter = spio.loadmat('stutter_all_c.mat')\n",
    "mat_all_c_fluent = spio.loadmat('fluent_all_c.mat')\n",
    "mat_all_c_stutter_label = spio.loadmat('subj_stut_all_c.mat')\n",
    "mat_all_c_fluent_label = spio.loadmat('subj_fluent_all_c.mat')\n",
    "\n",
    "mat_all_w_stutter = spio.loadmat('stutter_all_w.mat')\n",
    "mat_all_w_fluent = spio.loadmat('fluent_all_w.mat')\n",
    "mat_all_w_stutter_label = spio.loadmat('subj_stut_all_w.mat')\n",
    "mat_all_w_fluent_label = spio.loadmat('subj_fluent_all_w.mat')\n",
    "\n",
    "all_c_stutter = data_transform(np.nan_to_num(mat_all_c_stutter['stutter_all_c'][500:1250, :, :]))\n",
    "all_c_fluent = data_transform(np.nan_to_num(mat_all_c_fluent['fluent_all_c'][500:1250, :, :]))\n",
    "all_w_stutter = data_transform(np.nan_to_num(mat_all_w_stutter['stutter_all_w'][500:1250, :, :]))\n",
    "all_w_fluent = data_transform(np.nan_to_num(mat_all_w_fluent['fluent_all_w'][500:1250, :, :]))\n",
    "\n",
    "# all_c_stutter = data_transform(mat_all_c_stutter['stutter_all_c'][500:1250, :, :])\n",
    "# all_c_fluent = data_transform(np.nan_to_num(mat_all_c_fluent['fluent_all_c'][500:1250, :, :]))\n",
    "# all_w_stutter = data_transform(np.nan_to_num(mat_all_w_stutter['stutter_all_w'][500:1250, :, :]))\n",
    "# all_w_fluent = data_transform(np.nan_to_num(mat_all_w_fluent['fluent_all_w'][500:1250, :, :]))\n",
    "\n",
    "X_array_stutter = np.concatenate((all_c_stutter, all_w_stutter))\n",
    "Y_array_stutter = np.ones(X_array_stutter.shape[0])\n",
    "\n",
    "X_array_fluent = np.concatenate((all_c_fluent, all_w_fluent))\n",
    "Y_array_fluent = np.zeros(X_array_fluent.shape[0])\n",
    "\n",
    "X_array = np.concatenate((X_array_stutter, X_array_fluent))\n",
    "Y_array = np.concatenate((Y_array_stutter, Y_array_fluent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X_array).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_array.shape: \", X_array.shape)\n",
    "Y_hist = np.histogram(Y_array)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _StutterNet_C_S1S2. Functional API model.\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "clear_session()\n",
    "\n",
    "channels = 5\n",
    "timesteps = 750 # upsampled 58 fps\n",
    "\n",
    "inputs = Input(shape=(channels, timesteps))\n",
    "\n",
    "input_permute = Permute((1, 2), input_shape=(channels, timesteps))(inputs)\n",
    "input_reshape = Reshape((1, channels, timesteps))(input_permute)\n",
    "\n",
    "conv2d_1 = Conv2D(32, (1,channels), activation='linear', input_shape=(channels, timesteps), padding='same')(input_reshape)\n",
    "conv2d_1_bn = BatchNormalization()(conv2d_1)\n",
    "\n",
    "conv2d_2DW = DepthwiseConv2D((channels,1), use_bias=False, activation='linear', depth_multiplier=2, padding='valid', kernel_constraint=max_norm(1.))(conv2d_1_bn)\n",
    "conv2d_2DW_bn = BatchNormalization()(conv2d_2DW)\n",
    "conv2d_2DW_bn_act = Activation('elu')(conv2d_2DW_bn)\n",
    "\n",
    "conv2d_2DW_bn_act_avpool = AveragePooling2D((1,4))(conv2d_2DW_bn_act)\n",
    "conv2d_2DW_bn_act_avpool_dp = Dropout(rate=0.25)(conv2d_2DW_bn_act_avpool)\n",
    "\n",
    "conv2d_3Sep = SeparableConv2D(32, (1, 16), activation='linear', padding='same')(conv2d_2DW_bn_act_avpool_dp)\n",
    "conv2d_3Sep_bn = BatchNormalization()(conv2d_3Sep)\n",
    "conv2d_3Sep_bn_act = Activation('elu')(conv2d_3Sep_bn)\n",
    "\n",
    "conv2d_3Sep_bn_act_avgpool = AveragePooling2D((1,8))(conv2d_3Sep_bn_act)\n",
    "conv2d_3Sep_bn_act_avgpool_dp = Dropout(rate=0.25)(conv2d_3Sep_bn_act_avgpool)\n",
    "\n",
    "flatten_1 = Flatten()(conv2d_3Sep_bn_act_avgpool_dp)\n",
    "dense_1 = Dense(64, activation='elu', kernel_constraint=max_norm(0.5))(flatten_1)\n",
    "# dense_1_reshape = Reshape((64, 1))(dense_1)\n",
    "\n",
    "# lstm_1 = LSTM(64, input_shape=(64, 1))(dense_1_reshape)\n",
    "# lstm_1_act = Activation('elu')(lstm_1)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid', kernel_constraint=max_norm(0.25))(dense_1)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "# sgd = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_model(model, to_file='StutterNet_C.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff = np.arange(X_array.shape[0])\n",
    "np.random.shuffle(shuff)\n",
    "X_shuffled = X_array[shuff]\n",
    "Y_shuffled = Y_array[shuff]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_shuffled, Y_shuffled, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "# csv_log = CSVLogger(model_path + 'train.log')\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=40)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=15, min_lr=0.001)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=500, batch_size=512, verbose=2, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    shuffle=True,\n",
    "                    callbacks=[reduce_lr])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "test_output = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = np.round(np.clip(y_pred, 0, 1)).flatten()\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy: {:.2f}%, Recall: {:.3f}, Precision: {:.3f}, F1: {:.3f}\".format(test_output[1]*100, recall, precision, f1))\n",
    "\n",
    "if test_output[1] > 0.75:\n",
    "    model_name = '_StutterNet_C_eye_tracking_S1S2_'\n",
    "    model_path = '../trained_models/' + str(datetime.date.today()) + model_name + '{:.3f}'.format(test_output[1])[-3:]\n",
    "    print(\"Saving to: \", model_path + '.h5')\n",
    "    model.save(model_path + '.h5')\n",
    "    with open(model_path + '_history.pkl','wb') as f: pickle.dump(model.history.history, f)\n",
    "    with open(model_path + '_params.pkl','wb') as f: pickle.dump(model.history.params, f)\n",
    "\n",
    "##     with open(model_path + '_history.pkl','rb') as f: history2 = pickle.load(f)\n",
    "##     with open(model_path + '_params.pkl','rb') as f: params2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
