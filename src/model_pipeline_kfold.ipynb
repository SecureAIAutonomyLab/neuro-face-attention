{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input, Reshape, Permute\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AU definitions\n",
    "au_columns_name = {}\n",
    "au_columns_name['AU01_r'] = 'Inner brow raiser, upper'\n",
    "au_columns_name['AU02_r'] = 'Outer brow raiser, upper'\n",
    "au_columns_name['AU04_r'] = 'Brow lowerer, upper'\n",
    "au_columns_name['AU05_r'] = 'Upper lid raiser, upper'\n",
    "au_columns_name['AU06_r'] = 'Cheekraiser, upper'\n",
    "au_columns_name['AU07_r'] = 'Lid tightener, upper'\n",
    "au_columns_name['AU09_r'] = 'Nose wrinkler, lower'\n",
    "au_columns_name['AU10_r'] = 'Upper lip raiser, lower'\n",
    "au_columns_name['AU12_r'] = 'Lip corner puller, lower'\n",
    "au_columns_name['AU14_r'] = 'Dimpler, lower'\n",
    "au_columns_name['AU15_r'] = 'Lip corner depressor, lower'\n",
    "au_columns_name['AU17_r'] = 'Chin raiser, lower'\n",
    "au_columns_name['AU20_r'] = 'Lipstretcher, lower'\n",
    "au_columns_name['AU23_r'] = 'Lip tightener, lower'\n",
    "au_columns_name['AU25_r'] = 'Lips part, lower'\n",
    "au_columns_name['AU26_r'] = 'Jaw drop, lower'\n",
    "au_columns_name['AU45_r'] = 'Blink, upper'\n",
    "\n",
    "fps_df = pd.read_csv('../dataset/csv_labels/FPS_of_stutter_dataset.csv', index_col=0)\n",
    "max_fps = max(fps_df[\"FPS\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../dataset/pickled_datasets/X_array_corrected_downsampled.pkl','rb') as f: X_array = pickle.load(f)\n",
    "with open('../dataset/pickled_datasets/Y_array_corrected_downsampled.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('../dataset/pickled_datasets/X_array_corrected_upsampled.pkl','rb') as f: X_array = pickle.load(f)\n",
    "with open('../dataset/pickled_datasets/Y_array_corrected_upsampled.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subject = 982\n",
    "with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: X_array = pickle.load(f)\n",
    "with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: Y_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['942', '1131', '1196', '1214', '970', '971', '982']\n",
    "\n",
    "X_array = []\n",
    "Y_array = []\n",
    "\n",
    "if len(subjects) > 1:\n",
    "    for subject in subjects:\n",
    "        with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: X_array.append(pickle.load(f))\n",
    "        with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: Y_array.append(pickle.load(f))\n",
    "    X_array = np.vstack(X_array)\n",
    "    Y_array = np.hstack(Y_array)\n",
    "else:\n",
    "    with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: X_array = pickle.load(f)\n",
    "    with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_'+ str(subject) +'.pkl','rb') as f: Y_array = pickle.load(f)\n",
    "        \n",
    "print(X_array.shape, Y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_array.shape: \", X_array.shape)\n",
    "Y_hist = np.histogram(Y_array)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _StutterNet_C_S1S2. Functional API model.\n",
    "\n",
    "def define_model():\n",
    "    K.set_image_data_format('channels_first')\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    channels = 17\n",
    "    timesteps = 87 # upsampled 58 fps\n",
    "\n",
    "    inputs = Input(shape=(channels, timesteps))\n",
    "\n",
    "    input_permute = Permute((1, 2), input_shape=(channels, timesteps))(inputs)\n",
    "    input_reshape = Reshape((1, channels, timesteps))(input_permute)\n",
    "\n",
    "    conv2d_1 = Conv2D(64, (1,17), activation='linear', input_shape=(channels, timesteps), padding='same')(input_reshape)\n",
    "    conv2d_1_bn = BatchNormalization()(conv2d_1)\n",
    "\n",
    "    conv2d_2DW = DepthwiseConv2D((17,1), use_bias=False, activation='linear', depth_multiplier=2, padding='valid', kernel_constraint=max_norm(1.))(conv2d_1_bn)\n",
    "    conv2d_2DW_bn = BatchNormalization()(conv2d_2DW)\n",
    "    conv2d_2DW_bn_act = Activation('elu')(conv2d_2DW_bn)\n",
    "\n",
    "    conv2d_2DW_bn_act_avpool = AveragePooling2D((1,4))(conv2d_2DW_bn_act)\n",
    "    conv2d_2DW_bn_act_avpool_dp = Dropout(rate=0.25)(conv2d_2DW_bn_act_avpool)\n",
    "\n",
    "    conv2d_3Sep = SeparableConv2D(64, (1, 16), activation='linear', padding='same')(conv2d_2DW_bn_act_avpool_dp)\n",
    "    conv2d_3Sep_bn = BatchNormalization()(conv2d_3Sep)\n",
    "    conv2d_3Sep_bn_act = Activation('elu')(conv2d_3Sep_bn)\n",
    "\n",
    "    conv2d_3Sep_bn_act_avgpool = AveragePooling2D((1,8))(conv2d_3Sep_bn_act)\n",
    "    conv2d_3Sep_bn_act_avgpool_dp = Dropout(rate=0.25)(conv2d_3Sep_bn_act_avgpool)\n",
    "\n",
    "    flatten_1 = Flatten()(conv2d_3Sep_bn_act_avgpool_dp)\n",
    "    dense_1 = Dense(64, activation='elu', kernel_constraint=max_norm(0.5))(flatten_1)\n",
    "    # dense_1_reshape = Reshape((64, 1))(dense_1)\n",
    "\n",
    "    # lstm_1 = LSTM(64, input_shape=(64, 1))(dense_1_reshape)\n",
    "    # lstm_1_act = Activation('elu')(lstm_1)\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid', kernel_constraint=max_norm(0.25))(dense_1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "# model = define_model()\n",
    "# model.summary()\n",
    "# plot_model(model, to_file='StutterNet_C.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_array_, X_test, Y_array_, y_test = train_test_split(\n",
    "    X_array, Y_array, test_size=0.3, random_state=seed)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=15, min_lr=0.001)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "count = 1\n",
    "\n",
    "for train, test in kfold.split(X_array_, Y_array_):\n",
    "    print(\"Processing Fold \", count)\n",
    "    count += 1\n",
    "    model = define_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X_array[train], Y_array[train], \n",
    "                        epochs=100, batch_size=256, verbose=0, \n",
    "                        validation_data=(X_array[test], Y_array[test]),\n",
    "                        shuffle=True,\n",
    "                        callbacks=[reduce_lr])\n",
    "\n",
    "    plot_history(history)\n",
    "\n",
    "    test_output = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = np.round(np.clip(y_pred, 0, 1)).flatten()\n",
    "    precision = precision_score(y_test, y_pred_binary)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    print(\"Accuracy: {:.2f}%, Recall: {:.3f}, Precision: {:.3f}, F1: {:.3f}\".format(test_output[1]*100, recall, precision, f1))\n",
    "    \n",
    "    accuracies.append(test_output[1]*100)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    if test_output[1] > 0.8:\n",
    "        model_name = '_StutterNet_C_upscaled_S1S2_subject_' + str(subject) + '_'\n",
    "        model_path = '../trained_models/' + str(datetime.date.today()) + model_name + '{:.3f}'.format(test_output[1])[-3:]\n",
    "        print(\"Saving to: \", model_path + '.h5')\n",
    "        model.save(model_path + '.h5')\n",
    "        with open(model_path + '_history.pkl','wb') as f: pickle.dump(model.history.history, f)\n",
    "        with open(model_path + '_params.pkl','wb') as f: pickle.dump(model.history.params, f)\n",
    "\n",
    "    ##     with open(model_path + '_history.pkl','rb') as f: history2 = pickle.load(f)\n",
    "    ##     with open(model_path + '_params.pkl','rb') as f: params2 = pickle.load(f)\n",
    "\n",
    "print(\"Training Statistics\")\n",
    "metrics_df = pd.DataFrame.from_dict({'Accuracy': accuracies, 'F1': f1s, 'Recall': recalls, 'Precision': precisions})\n",
    "print(metrics_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
