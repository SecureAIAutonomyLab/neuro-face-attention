{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "# AU definitions\n",
    "au_columns_name = {}\n",
    "au_columns_name['AU01_r'] = 'Inner brow raiser, upper'\n",
    "au_columns_name['AU02_r'] = 'Outer brow raiser, upper'\n",
    "au_columns_name['AU04_r'] = 'Brow lowerer, upper'\n",
    "au_columns_name['AU05_r'] = 'Upper lid raiser, upper'\n",
    "au_columns_name['AU06_r'] = 'Cheekraiser, upper'\n",
    "au_columns_name['AU07_r'] = 'Lid tightener, upper'\n",
    "au_columns_name['AU09_r'] = 'Nose wrinkler, lower'\n",
    "au_columns_name['AU10_r'] = 'Upper lip raiser, lower'\n",
    "au_columns_name['AU12_r'] = 'Lip corner puller, lower'\n",
    "au_columns_name['AU14_r'] = 'Dimpler, lower'\n",
    "au_columns_name['AU15_r'] = 'Lip corner depressor, lower'\n",
    "au_columns_name['AU17_r'] = 'Chin raiser, lower'\n",
    "au_columns_name['AU20_r'] = 'Lipstretcher, lower'\n",
    "au_columns_name['AU23_r'] = 'Lip tightener, lower'\n",
    "au_columns_name['AU25_r'] = 'Lips part, lower'\n",
    "au_columns_name['AU26_r'] = 'Jaw drop, lower'\n",
    "au_columns_name['AU45_r'] = 'Blink, upper'\n",
    "\n",
    "def generate_dataset(subjects, studies, paradigms, stack=False):\n",
    "    \"\"\"Generate Dataset based on resampling method for creating a balanced dataset with the action unit information.\n",
    "    \"\"\"\n",
    "    fps_dataset = pd.read_csv('../dataset/csv_labels/FPS_of_stutter_dataset.csv', index_col=0)\n",
    "    \n",
    "    for subject in subjects:\n",
    "        for study in studies:\n",
    "            for paradigm in paradigms:\n",
    "                global X\n",
    "                global Y\n",
    "                \n",
    "                print(\"{}/{}/{}\".format(subject, study, paradigm))\n",
    "                fps = int(fps_dataset[np.logical_and(np.logical_and(fps_dataset['Paradigm']=='CAW1', fps_dataset['Study']=='S1'), fps_dataset['Subject']==982)][\"FPS\"])\n",
    "                max_fps = int(max(fps_dataset['FPS']))\n",
    "                \n",
    "                if os.path.exists(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.lower(), ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv'):\n",
    "                    df_with_trials = pd.read_csv(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.lower(), ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv', sep=',')\n",
    "                elif os.path.exists(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.upper(), ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv'):\n",
    "                    df_with_trials = pd.read_csv(('/').join(['../dataset', 'csv_labels', subject, study, paradigm.upper(), ('_').join(['trial', 'frames', subject, study.lower(), paradigm.lower()])]) + '.csv')\n",
    " \n",
    "                if os.path.exists(('/').join(['../dataset', 'au', subject, study, paradigm.lower(), ('_').join([subject, paradigm.lower()])]) + '.csv'):\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.lower(), ('_').join([subject, paradigm.lower()])]) + '.csv', sep=', ')\n",
    "                elif os.path.exists(('/').join(['../dataset', 'au', subject, study, paradigm.upper(), ('_').join([subject, paradigm.upper()])]) + '.csv'):\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.upper(), ('_').join([subject, paradigm.upper()])]) + '.csv', sep=', ')\n",
    "                elif os.path.exists(('/').join(['../dataset', 'au', subject, study, paradigm.lower(), ('_').join([subject, paradigm.upper()])]) + '.csv'):\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.lower(), ('_').join([subject, paradigm.upper()])]) + '.csv', sep=', ')\n",
    "                elif os.path.exists(('/').join(['../dataset', 'au', subject, study, paradigm.upper(), ('_').join([subject, paradigm.lower()])]) + '.csv'):\n",
    "                    full_au_dataset = pd.read_csv(('/').join(['../dataset', 'au', subject, study, paradigm.upper(), ('_').join([subject, paradigm.lower()])]) + '.csv', sep=', ')\n",
    "                \n",
    "                #print(full_au_dataset.head())\n",
    "                just_aus = full_au_dataset[sorted(list(au_columns_name.keys()))]\n",
    "                # print(just_aus.head())\n",
    "                just_aus['sequence'] = just_aus.apply(lambda x: list(x), axis=1)\n",
    "                # just_aus.head()\n",
    "\n",
    "                # minimum number of sequences found from the dataset csv.\n",
    "                min_length = min(df_with_trials['trial_end'] - df_with_trials['trial_start'])\n",
    "                # for all 50 trials per subject\n",
    "                for i in range(50):\n",
    "                    S1 = df_with_trials['trial_start'][i]\n",
    "                    S2 = int(S1 + 1.5*fps)\n",
    "                    signal_df = pd.DataFrame(list(just_aus['sequence'].iloc[S1:S2]))\n",
    "                    signal_df_resampled = signal_df.apply(lambda x: signal.resample(x, int(1.5*max_fps)), axis=0)\n",
    "\n",
    "                    # Round the values to 3 decimals (ablation study for preprocessing maybe?)\n",
    "                    signal_df_resampled = np.round(signal_df_resampled, 3)\n",
    "                    # Convert to numpy array\n",
    "                    signal_array = np.asarray(np.round(signal_df_resampled, 3), np.float32)\n",
    "\n",
    "                    X.append(signal_array)\n",
    "                    Y.append(int(df_with_trials['code'].iloc[i]))\n",
    "                    \n",
    "    if stack:\n",
    "        # Moving axes to DL format NHWC. However, here, we don't consider 'c (channels)'\n",
    "        # because we do the permute and reshape operations during training in Keras.\n",
    "        # Not the best way to do it, but we can always add code to create the dataset properly\n",
    "        # to work with any DL platform. As such the only change needed to this X array\n",
    "        # is to add a new axis for the channels and make it 4D NHWC or NCHW.\n",
    "        X = np.dstack(X)\n",
    "        X = np.moveaxis(X, [0,1], [2,1])\n",
    "\n",
    "def stack_dataset(X):\n",
    "    # Moving axes to DL format NHWC. However, here, we don't consider 'c (channels)'\n",
    "    # because we do the permute and reshape operations during training in Keras.\n",
    "    # Not the best way to do it, but we can always add code to create the dataset properly\n",
    "    # to work with any DL platform. As such the only change needed to this X array\n",
    "    # is to add a new axis for the channels and make it 4D NHWC or NCHW.\n",
    "    X = np.dstack(X)\n",
    "    X = np.moveaxis(X, [0,1], [2,1])\n",
    "    return (X)\n",
    "\n",
    "def correct_dataset_labels(_X, _Y):\n",
    "    list_of_unwanted_indexes = []\n",
    "    for i, y in enumerate(_Y):\n",
    "        # print (i,y)\n",
    "        if y==0:\n",
    "            _Y[i] = 0\n",
    "        if 0 < y < 4:\n",
    "            # print(y, \"Y[i] between 0 and 4. Y[i] = 1\")\n",
    "            _Y[i] = 1\n",
    "        elif y>3:\n",
    "            list_of_unwanted_indexes.append(i)    \n",
    "\n",
    "    _X_ = [v for i,v in enumerate(_X) if i not in list_of_unwanted_indexes]\n",
    "    _Y_ = [v for i,v in enumerate(_Y) if i not in list_of_unwanted_indexes]\n",
    "    return(np.asarray(_X_), np.asarray(_Y_))\n",
    "\n",
    "def save_data_to_file(subjects, studies, paradigms, X_array, X_array_corrected, X_array_balanced, Y_array, Y_array_corrected, Y_array_balanced):\n",
    "    if any(p in paradigms for p in ['caw1', 'caw2', 'cw1', 'cw2']) and any(p in paradigms for p in ['wag1', 'wag2', 'wg1', 'wg2']):\n",
    "        # raw data\n",
    "        with open('../dataset/pickled_datasets/X_array_raw_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_raw_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array, np.float32), f)\n",
    "        # label corrected data\n",
    "        with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_corrected, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_corrected, np.float32), f)\n",
    "        # balanced data\n",
    "        with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "        print(\"Saved all paradigms.. \")\n",
    "    elif any(p in paradigms for p in ['caw1', 'caw2', 'cw1', 'cw2']):\n",
    "        # raw data\n",
    "        with open('../dataset/pickled_datasets/X_array_raw_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_raw_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array, np.float32), f)\n",
    "        # label corrected data\n",
    "        with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_corrected, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_corrected, np.float32), f)\n",
    "        # balanced data\n",
    "        with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_CueWord_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "        print(\"Saved CW paradigms.. \")\n",
    "    elif any(p in paradigms for p in ['wag1', 'wag2', 'wg1', 'wg2']):\n",
    "        # raw data\n",
    "        with open('../dataset/pickled_datasets/X_array_raw_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_raw_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array, np.float32), f)\n",
    "        # label corrected data\n",
    "        with open('../dataset/pickled_datasets/X_array_corrected_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_corrected, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_corrected_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_corrected, np.float32), f)\n",
    "        # balanced data\n",
    "        with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "        with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_WordGo_' + str(subjects[0]) + '.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "        print(\"Saved WG paradigms.. \")\n",
    "    else:\n",
    "        raise ValueError('Files not saved !!!')\n",
    "\n",
    "def generate_single_subject(subject, studies, paradigms, save=True):\n",
    "    \"\"\"\n",
    "    ex: generate_single_subject(['942'], ['S1', 'S2'], ['caw1', 'caw2', 'cw1', 'cw2'])\n",
    "    \"\"\"\n",
    "    subjects = subject\n",
    "    generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "    # stack datasets\n",
    "    X_array = stack_dataset(X)\n",
    "    Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "    # Create label corrected dataset\n",
    "    X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "    Y_hist = np.histogram(Y_array_corrected)\n",
    "    Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "    print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "    print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "    try:\n",
    "        # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "        # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "        # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "        # later we concatenate both data points to create a balanced dataset.\n",
    "        random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "        assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "        random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "        random.shuffle(random_data_points)\n",
    "\n",
    "    except ValueError:\n",
    "        # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "        random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "        assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "        random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "        random.shuffle(random_data_points)\n",
    "\n",
    "    X_array_balanced = X_array[random_data_points]\n",
    "    Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "    print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "    \n",
    "    if save:\n",
    "        save_data_to_file(subjects, studies, paradigms, X_array, X_array_corrected, X_array_balanced, Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm_type = 'all'\n",
    "\n",
    "# Subject 942\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['942']\n",
    "studies = ['S1', 'S2']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 970\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['970']\n",
    "studies = ['S5']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 971\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['971']\n",
    "studies = ['S3', 'S5']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "subjects = ['971']\n",
    "studies = ['S4']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'wag1', 'wag2', 'wg1', 'wg2'] # no frames in CW2\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 982\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "subjects = ['982']\n",
    "studies = ['S4']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1', 'caw2', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2'] # no frames in WAG1\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "subjects = ['982']\n",
    "studies = ['S3', 'S5']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "subjects = ['982']\n",
    "studies = ['S1']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1', 'caw2', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2'] # word mismatch in WAG1\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "subjects = ['982']\n",
    "studies = ['S2']\n",
    "if paradigm_type == 'all': # no frames in CAW1, CW1, and WG2\n",
    "    paradigms = ['caw2', 'cw2', 'wag1', 'wag2', 'wg1']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw2', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1131\n",
    "paradigm_type = 'all'\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['1131']\n",
    "studies = ['S1', 'S2', 'S3', 'S4', 'S5']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1196\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['1196']\n",
    "studies = ['S5', 'S4', 'S3', 'S1', 'S2']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1214\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "subjects = ['1214']\n",
    "studies = ['S1']\n",
    "if paradigm_type == 'all':\n",
    "    paradigms = ['caw1','caw2', 'wag1', 'wag2', 'cw1', 'cw2', 'wg1', 'wg2']\n",
    "elif paradigm_type == 'cw':\n",
    "    paradigms = ['caw1', 'caw2', 'cw1', 'cw2']\n",
    "elif paradigm_type == 'wg':\n",
    "    paradigms = ['wag1', 'wag2', 'wg1', 'wg2']\n",
    "generate_dataset(subjects, studies, paradigms)\n",
    "\n",
    "\n",
    "# stack datasets\n",
    "X_array = stack_dataset(X)\n",
    "Y_array = np.asarray(Y, dtype=np.int32)\n",
    "\n",
    "# Create label corrected dataset\n",
    "X_array_corrected, Y_array_corrected = correct_dataset_labels(X_array, Y_array)\n",
    "Y_hist = np.histogram(Y_array_corrected)\n",
    "Y_hist_sum = Y_hist[0][0] + Y_hist[0][-1]\n",
    "print(\"Fluent Trials: {} ({:.2f}%), Stutter Trials: {} ({:.2f}%)\".format(Y_hist[0][0], 100*(Y_hist[0][0]/Y_hist_sum), Y_hist[0][-1], 100*(Y_hist[0][-1]/Y_hist_sum)))\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "\n",
    "print(\"Balanced Shapes X:\", X_array_balanced.shape, \"Y:\", Y_array_balanced.shape)\n",
    "save_data_to_file(subjects, studies, paradigms, \n",
    " X_array, X_array_corrected, X_array_balanced, \n",
    " Y_array, Y_array_corrected, Y_array_balanced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
