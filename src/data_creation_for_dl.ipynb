{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_type = \"corrected\"\n",
    "subjects = [942, 970, 971, 982, 1131, 1196, 1214]\n",
    "\n",
    "X_array = []\n",
    "Y_array = []\n",
    "for subject in subjects:\n",
    "    with open('../dataset/pickled_datasets/X_array_'+process_type+'_upsampled_S1S2_'+str(subject)+'.pkl', 'rb') as f: X_array.append(pickle.load(f))\n",
    "    with open('../dataset/pickled_datasets/Y_array_'+process_type+'_upsampled_S1S2_'+str(subject)+'.pkl', 'rb') as f: Y_array.append(pickle.load(f))\n",
    "X_array = np.vstack(X_array)\n",
    "Y_array = np.hstack(Y_array)\n",
    "print(X_array.shape, Y_array.shape)\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "print(X_array_balanced.shape, Y_array_balanced.shape)\n",
    "\n",
    "with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "print(\"Saved all paradigms.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_type = \"corrected\"\n",
    "subjects = [942, 970, 971, 982, 1131, 1196, 1214]\n",
    "\n",
    "X_array = []\n",
    "Y_array = []\n",
    "for subject in subjects:\n",
    "    with open('../dataset/pickled_datasets/X_array_'+process_type+'_upsampled_S1S2_CueWord_'+str(subject)+'.pkl', 'rb') as f: X_array.append(pickle.load(f))\n",
    "    with open('../dataset/pickled_datasets/Y_array_'+process_type+'_upsampled_S1S2_CueWord_'+str(subject)+'.pkl', 'rb') as f: Y_array.append(pickle.load(f))\n",
    "X_array = np.vstack(X_array)\n",
    "Y_array = np.hstack(Y_array)\n",
    "print(X_array.shape, Y_array.shape)\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "print(X_array_balanced.shape, Y_array_balanced.shape)\n",
    "\n",
    "with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_CueWord_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_CueWord_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "print(\"Saved all paradigms.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_type = \"corrected\"\n",
    "subjects = [942, 970, 971, 982, 1131, 1196, 1214]\n",
    "\n",
    "X_array = []\n",
    "Y_array = []\n",
    "for subject in subjects:\n",
    "    with open('../dataset/pickled_datasets/X_array_'+process_type+'_upsampled_S1S2_WordGo_'+str(subject)+'.pkl', 'rb') as f: X_array.append(pickle.load(f))\n",
    "    with open('../dataset/pickled_datasets/Y_array_'+process_type+'_upsampled_S1S2_WordGo_'+str(subject)+'.pkl', 'rb') as f: Y_array.append(pickle.load(f))\n",
    "X_array = np.vstack(X_array)\n",
    "Y_array = np.hstack(Y_array)\n",
    "print(X_array.shape, Y_array.shape)\n",
    "\n",
    "print(\"Creating Balanced Dataset with 50-50 Split\")\n",
    "try:\n",
    "    # more fluent than stutter trials. choose randomly as much stutter trials from the fluent and concat.\n",
    "    # np.where(Y_array==0) will give the indices where the array is 0 (fluent).\n",
    "    # then using np.random.choice, we choose X_array[Y_array==1].shape[0] (stutter) number of samples from the fluent trials.\n",
    "    # later we concatenate both data points to create a balanced dataset.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==0)[0], size=X_array[Y_array==1].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==1].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==1)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "    \n",
    "except ValueError:\n",
    "    # more stutter than fluent trials. choose randomly as much fluent trials from the stutter and concat.\n",
    "    random_data_points = np.random.choice(np.where(Y_array==1)[0], size=X_array[Y_array==0].shape[0], replace=False)\n",
    "    assert random_data_points.shape[0] == X_array[Y_array==0].shape[0]\n",
    "    random_data_points = np.concatenate((random_data_points, np.where(Y_array==0)[0]))\n",
    "    random.shuffle(random_data_points)\n",
    "\n",
    "X_array_balanced = X_array[random_data_points]\n",
    "Y_array_balanced = Y_array[random_data_points]\n",
    "print(X_array_balanced.shape, Y_array_balanced.shape)\n",
    "\n",
    "with open('../dataset/pickled_datasets/X_array_balanced_upsampled_S1S2_WordGo_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(X_array_balanced, np.float32), f)\n",
    "with open('../dataset/pickled_datasets/Y_array_balanced_upsampled_S1S2_WordGo_all_subjects.pkl','wb') as f: pickle.dump(np.asarray(Y_array_balanced, np.float32), f)\n",
    "print(\"Saved all paradigms.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
